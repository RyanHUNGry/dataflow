{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook documents computation of summary statistics from a dataset read from S3 bucket\n",
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# No need to specify access keys due to CLI authentication existing and Lambda does not need access keys\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Bucket and key configuration\n",
    "BUCKET = 'dataflow-summary-statistics-bucket'\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    DATASET_KEY = event[\"Records\"][0]['s3']['object']['key']\n",
    "    SUMMARY_STATISTICS_KEY = 'summary/' + (DATASET_KEY.split('/')[1].split('.csv')[0]) + '.json'\n",
    "\n",
    "    # This reads the object stream and decodes into a string to be read as a CSV into pd dataframe\n",
    "    csv = s3.get_object(Bucket='dataflow-development-bucket', Key=DATASET_KEY)['Body'].read().decode('utf-8')\n",
    "    csv = io.StringIO(csv)\n",
    "    df = pd.read_csv(csv)\n",
    "\n",
    "    response = {}\n",
    "\n",
    "    # Compute numeric summary statistics first\n",
    "    summary = df.select_dtypes(include=['number']).describe()\n",
    "    for col in summary.columns:\n",
    "        response[col] = dict(summary[col])\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in summary.columns:\n",
    "            response[col] = {\n",
    "                'avg_length': df[col].str.len().mean(), # Average length\n",
    "                'avg_words': df[col].str.split(' ').str.len().mean(), # Average words\n",
    "                'avg_capitals': df[col].str.count(pat='[A-Z]').mean(), # Average capitals\n",
    "                'avg_symbols': df[col].str.count(pat='[^a-zA-Z0-9\\s]').mean() # Average symbols\n",
    "            }\n",
    "\n",
    "    json_data = json.dumps(response)\n",
    "\n",
    "    # Upload to s3 summary statistics bucket\n",
    "    response = s3.put_object(Key=SUMMARY_STATISTICS_KEY, Bucket=BUCKET, Body=json_data)\n",
    "\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "        print('Summary statistics successfully computed and uploaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Records': [{'eventVersion': '2.1',\n",
       "   'eventSource': 'aws:s3',\n",
       "   'awsRegion': 'us-west-1',\n",
       "   'eventTime': '2023-09-02T03:13:17.071Z',\n",
       "   'eventName': 'ObjectCreated:Put',\n",
       "   'userIdentity': {'principalId': 'AWS:AIDAT5UFTAUENG3VC6BG4'},\n",
       "   'requestParameters': {'sourceIPAddress': '73.241.55.230'},\n",
       "   'responseElements': {'x-amz-request-id': '23M5Q21SR7TSB9M3',\n",
       "    'x-amz-id-2': 'gQEtKKIEBihslEQIYBhIQ0ASFFErCiI+9LaItdrdUxeDfPYmbdO/eD8N3/z2QM/Dn1rXPsxgznCn5hrmdehUjOhB0zpP9fCG'},\n",
       "   's3': {'s3SchemaVersion': '1.0',\n",
       "    'configurationId': 'd0807870-565d-460c-b4f8-b7fd2a6e151a',\n",
       "    'bucket': {'name': 'dataflow-development-bucket',\n",
       "     'ownerIdentity': {'principalId': 'A4GK0NDNF3RJ3'},\n",
       "     'arn': 'arn:aws:s3:::dataflow-development-bucket'},\n",
       "    'object': {'key': 'datasets/test.csv',\n",
       "     'size': 89,\n",
       "     'eTag': '6e1905bb12538fc6d234be99c40e1193',\n",
       "     'sequencer': '0064F2A84D0CB3501C'}}}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This documents the request object when a dataset is uploaded to s3 for lambda\n",
    "lambda_event = {'Records': [{'eventVersion': '2.1', 'eventSource': 'aws:s3', 'awsRegion': 'us-west-1', 'eventTime': '2023-09-02T03:13:17.071Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'AWS:AIDAT5UFTAUENG3VC6BG4'}, 'requestParameters': {'sourceIPAddress': '73.241.55.230'}, 'responseElements': {'x-amz-request-id': '23M5Q21SR7TSB9M3', 'x-amz-id-2': 'gQEtKKIEBihslEQIYBhIQ0ASFFErCiI+9LaItdrdUxeDfPYmbdO/eD8N3/z2QM/Dn1rXPsxgznCn5hrmdehUjOhB0zpP9fCG'}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'd0807870-565d-460c-b4f8-b7fd2a6e151a', 'bucket': {'name': 'dataflow-development-bucket', 'ownerIdentity': {'principalId': 'A4GK0NDNF3RJ3'}, 'arn': 'arn:aws:s3:::dataflow-development-bucket'}, 'object': {'key': 'datasets/test.csv', 'size': 89, 'eTag': '6e1905bb12538fc6d234be99c40e1193', 'sequencer': '0064F2A84D0CB3501C'}}}]}\n",
    "lambda_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics successfully computed and uploaded\n"
     ]
    }
   ],
   "source": [
    "# This invokes the lambda handler and simulates a lambda trigger\n",
    "lambda_handler(lambda_event, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
